from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from MLXEngine import MLXLLM
import json
import asyncio
import uvicorn
from contextlib import asynccontextmanager

# Shared state
model_ready = False
llm = None

# Global stop signal
stop_stream = False



@asynccontextmanager
async def lifespan(app: FastAPI):
    global llm, model_ready
    print("üîÑ Loading MLX model...")
    mlx_engine = MLXLLM()
    llm = await mlx_engine.load_model()
    model_ready = True
    print("‚úÖ Model loaded and ready.")
    yield
    
    

app = FastAPI(lifespan=lifespan)



# Data model for the /chat endpoint
class ChatRequest(BaseModel):
    input: str
    
@app.get("/status")
def status():
    return {"ready": model_ready}

@app.get("/")
def read_root():
    return {"message": "Welcome To Suri Ai, Experience the Power of Offline Personal Assistant"}



@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    global stop_stream

    user_message = request.input
    stop_stream = False  # Reset stop before new chat

    async def generate_sse():
        global stop_stream
        try:
            for chunk in llm.stream(user_message):
                if stop_stream:
                    print("üõë Stream manually stopped")
                    break
                json_chunk = json.dumps({"content": chunk.content})
                yield f"data: {json_chunk}\n\n"
                await asyncio.sleep(0)  # allow cancellation
        except asyncio.CancelledError:
            print("‚ö†Ô∏è Streaming cancelled")
            raise

    return StreamingResponse(generate_sse(), media_type="text/event-stream")


@app.post("/stop")
async def stop_generation():
    global stop_stream
    stop_stream = True
    return {"status": "stopping"}




if __name__ == "__main__":
    
    uvicorn.run(app, host="127.0.0.1", port=8000)